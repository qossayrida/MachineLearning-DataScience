{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T08:14:14.116778348Z",
     "start_time": "2024-11-18T08:14:14.075499653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4979 entries, 0 to 4978\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   car name         4979 non-null   object \n",
      " 1   engine_capacity  4979 non-null   float64\n",
      " 2   cylinder         4979 non-null   int64  \n",
      " 3   horse_power      4979 non-null   int64  \n",
      " 4   top_speed        4979 non-null   int64  \n",
      " 5   brand            4979 non-null   object \n",
      " 6   country          4979 non-null   object \n",
      " 7   num_seats        4979 non-null   int64  \n",
      " 8   Price_USD        4979 non-null   float64\n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 350.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/processed_cars_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4979 entries, 0 to 4978\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   engine_capacity  4979 non-null   float64\n",
      " 1   cylinder         4979 non-null   int64  \n",
      " 2   horse_power      4979 non-null   int64  \n",
      " 3   top_speed        4979 non-null   int64  \n",
      " 4   num_seats        4979 non-null   int64  \n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 194.6 KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "X = df.drop(columns=[\"Price_USD\", \"car name\",'brand', 'country'])  # Drop 'Price_USD' (target) and 'car name' (irrelevant feature)\n",
    "y = df[\"Price_USD\"]\n",
    "\n",
    "# # One-hot encode categorical features (brand, country)\n",
    "# X = pd.get_dummies(X, columns=[], drop_first=True)\n",
    "# X = X.astype(np.float64)\n",
    "\n",
    "# Splitting the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add bias column to scaled datasets\n",
    "X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_val_b = np.c_[np.ones((X_val_scaled.shape[0], 1)), X_val_scaled]\n",
    "X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "X.info()\n",
    "model_results = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T08:14:14.117064961Z",
     "start_time": "2024-11-18T08:14:14.116539102Z"
    }
   },
   "id": "3a561df254a3c68b"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent - Validation Metrics: MSE = 6736827576.766, MAE = 27708.579, R² = 0.353\n",
      "Gradient Descent - Test Metrics: MSE = 16650933020.589, MAE = 31401.406, R² = 0.303\n",
      "Gradient Descent - Coefficients: [ 7.12590700e+04 -5.89216503e+01  1.16963734e+04  4.77025659e+04\n",
      "  1.55275470e+04 -3.05710801e+03]\n",
      "\n",
      "Closed-form Solution - Validation Metrics: MSE = 6886305765.176, MAE = 27971.329, R² = 0.338\n",
      "Closed-form Solution - Test Metrics: MSE = 16765571767.523, MAE = 31688.159, R² = 0.298\n",
      "Closed-form Solution - Coefficients: [71262.14649146   -72.98585132  9613.45568639 50250.67310241\n",
      " 14856.60315043 -2976.84142149]\n"
     ]
    }
   ],
   "source": [
    "# Closed-form solution & Gradient Descent With comparison between the two methods \n",
    "\n",
    "try:\n",
    "    ##############################\n",
    "    #### Run gradient descent ####\n",
    "    ##############################\n",
    "\n",
    "    theta_best_descent = np.zeros(X_train_b.shape[1])  # Includes bias term  \n",
    "    # Gradient Descent Implementation\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "    m = X_train_b.shape[0]  # Number of samples in training set\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gradients = (1 / m) * X_train_b.T @ (X_train_b @ theta_best_descent - y_train)  # Compute gradients\n",
    "        theta_best_descent -= learning_rate * gradients  # Update theta\n",
    "    \n",
    "    # Predictions on validation set\n",
    "    y_val_pred_gd = X_val_b @ theta_best_descent\n",
    "    \n",
    "    # Metrics for Gradient Descent\n",
    "    mse_gd = np.mean((y_val - y_val_pred_gd) ** 2)\n",
    "    mae_gd = np.mean(np.abs(y_val - y_val_pred_gd))\n",
    "    ss_total_gd = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    ss_residual_gd = np.sum((y_val - y_val_pred_gd) ** 2)\n",
    "    r2_gd = 1 - (ss_residual_gd / ss_total_gd)\n",
    "    \n",
    "    # Test Set Metrics\n",
    "    y_test_pred_gd = X_test_b @ theta_best_descent\n",
    "    mse_gd_test = np.mean((y_test - y_test_pred_gd) ** 2)\n",
    "    mae_gd_test = np.mean(np.abs(y_test - y_test_pred_gd))\n",
    "    r2_gd_test = 1 - (np.sum((y_test - y_test_pred_gd) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "    \n",
    "    print(f\"Gradient Descent - Validation Metrics: MSE = {mse_gd:.3f}, MAE = {mae_gd:.3f}, R² = {r2_gd:.3f}\")\n",
    "    print(f\"Gradient Descent - Test Metrics: MSE = {mse_gd_test:.3f}, MAE = {mae_gd_test:.3f}, R² = {r2_gd_test:.3f}\")\n",
    "    print(\"Gradient Descent - Coefficients:\", theta_best_descent)\n",
    "    \n",
    "    ##############################\n",
    "    #### Closed-form solution ####\n",
    "    ##############################\n",
    "    theta_best_closed = np.linalg.inv(X_train_b.T @ X_train_b) @ X_train_b.T @ y_train\n",
    "    y_val_pred_closed = X_val_b @ theta_best_closed\n",
    "    \n",
    "    # Metrics for Closed-form Solution\n",
    "    mse_closed = np.mean((y_val - y_val_pred_closed) ** 2)\n",
    "    mae_closed = np.mean(np.abs(y_val - y_val_pred_closed))\n",
    "    ss_total_closed = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    ss_residual_closed = np.sum((y_val - y_val_pred_closed) ** 2)\n",
    "    r2_closed = 1 - (ss_residual_closed / ss_total_closed)\n",
    "    \n",
    "    #Test Set Metrics\n",
    "    y_test_pred_closed = X_test_b @ theta_best_closed\n",
    "    mse_closed_test = np.mean((y_test - y_test_pred_closed) ** 2)\n",
    "    mae_closed_test = np.mean(np.abs(y_test - y_test_pred_closed))\n",
    "    r2_closed_test = 1 - (np.sum((y_test - y_test_pred_closed) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "    \n",
    "    print(f\"\\nClosed-form Solution - Validation Metrics: MSE = {mse_closed:.3f}, MAE = {mae_closed:.3f}, R² = {r2_closed:.3f}\")\n",
    "    print(f\"Closed-form Solution - Test Metrics: MSE = {mse_closed_test:.3f}, MAE = {mae_closed_test:.3f}, R² = {r2_closed_test:.3f}\")\n",
    "    print(\"Closed-form Solution - Coefficients:\", theta_best_closed)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Error: Singular matrix. Unable to compute the closed-form solution.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T08:14:14.171849407Z",
     "start_time": "2024-11-18T08:14:14.116669478Z"
    }
   },
   "id": "6219790e81045bfa"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO - Validation Metrics: MSE= 6884149445.228, MAE= 27933.368, R²= 0.339\n",
      "LASSO - Test Metrics: MSE= 16768004847.356, MAE= 31647.551, R²= 0.298\n",
      "LASSO - Coefficients: [   -0.          9554.38939557 50231.92653826 14823.27287075\n",
      " -2900.2790271 ]\n",
      "\n",
      "Ridge - Validation Metrics: MSE= 6676014578.626, MAE= 27253.356, R²= 0.359\n",
      "Ridge - Test Metrics: MSE= 16642234451.202, MAE= 30964.481, R²= 0.303\n",
      "Ridge - Coefficients: [  -59.7163599  11570.24263223 46837.48222636 15380.19623224\n",
      " -3190.66651936]\n"
     ]
    }
   ],
   "source": [
    "# LASSO and Ridge Regression with hyperparameter tuning and evaluation\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "############################################ \n",
    "###   LASSO Regression with Grid Search  ###\n",
    "############################################ \n",
    "lasso = Lasso(max_iter=10000)\n",
    "lasso_grid = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "\n",
    "############################################ \n",
    "####  Ridge Regression with Grid Search  ###\n",
    "############################################ \n",
    "ridge = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "\n",
    "################################# \n",
    "###   Validation Set Metrics  ###\n",
    "################################# \n",
    "# LASSO Predictions\n",
    "lasso_val_pred = best_lasso.predict(X_val_scaled)\n",
    "lasso_mse = mean_squared_error(y_val, lasso_val_pred)\n",
    "lasso_mae = mean_absolute_error(y_val, lasso_val_pred)\n",
    "lasso_r2 = r2_score(y_val, lasso_val_pred)\n",
    "\n",
    "# Ridge Predictions\n",
    "ridge_val_pred = best_ridge.predict(X_val_scaled)\n",
    "ridge_mse = mean_squared_error(y_val, ridge_val_pred)\n",
    "ridge_mae = mean_absolute_error(y_val, ridge_val_pred)\n",
    "ridge_r2 = r2_score(y_val, ridge_val_pred)\n",
    "\n",
    "############################# \n",
    "###  Test Set Evaluation  ###\n",
    "############################# \n",
    "# LASSO Predictions\n",
    "lasso_test_pred = best_lasso.predict(X_test_scaled)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_test_pred)\n",
    "lasso_test_mae = mean_absolute_error(y_test, lasso_test_pred)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_test_pred)\n",
    "\n",
    "# Ridge Predictions\n",
    "ridge_test_pred = best_ridge.predict(X_test_scaled)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_test_pred)\n",
    "ridge_test_mae = mean_absolute_error(y_test, ridge_test_pred)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_test_pred)\n",
    "\n",
    "print(f\"LASSO - Validation Metrics: MSE= {lasso_mse:.3f}, MAE= {lasso_mae:.3f}, R²= {lasso_r2:.3f}\")\n",
    "print(f\"LASSO - Test Metrics: MSE= {lasso_test_mse:.3f}, MAE= {lasso_test_mae:.3f}, R²= {lasso_test_r2:.3f}\")\n",
    "print(\"LASSO - Coefficients:\", best_lasso.coef_)\n",
    "\n",
    "print(f\"\\nRidge - Validation Metrics: MSE= {ridge_mse:.3f}, MAE= {ridge_mae:.3f}, R²= {ridge_r2:.3f}\")\n",
    "print(f\"Ridge - Test Metrics: MSE= {ridge_test_mse:.3f}, MAE= {ridge_test_mae:.3f}, R²= {ridge_test_r2:.3f}\")\n",
    "print(\"Ridge - Coefficients:\", best_ridge.coef_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T08:14:14.256543110Z",
     "start_time": "2024-11-18T08:14:14.175931748Z"
    }
   },
   "id": "c10704a95e243285"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Results:\n",
      "Degree 2 - validation : MSE = 136747505007.844, MAE = 30538.249, R² = -12.138\n",
      "Degree 2 - test : MSE = 141527871852.193, MAE = 33183.556, R² = -4.927\n",
      "Degree 3 - validation : MSE = 7935693269080.044, MAE = 106412.882, R² = -761.415\n",
      "Degree 3 - test : MSE = 14704100285247.975, MAE = 189711.805, R² = -614.803\n",
      "Degree 4 - validation : MSE = 69018415700815168.000, MAE = 8340790.995, R² = -6630880.729\n",
      "Degree 4 - test : MSE = 305594180640714560.000, MAE = 23755691.122, R² = -12798178.574\n",
      "Degree 5 - validation : MSE = 256435626219505647616.000, MAE = 507426844.230, R² = -24636820351.798\n",
      "Degree 5 - test : MSE = 2548493014005957787648.000, MAE = 2024481915.050, R² = -106730014190.660\n",
      "Degree 6 - validation : MSE = 40339177622382550450176.000, MAE = 6364173838.010, R² = -3875549926168.537\n",
      "Degree 6 - test : MSE = 229666195368673441480704.000, MAE = 20777084589.084, R² = -9618341567477.855\n",
      "Degree 7 - validation : MSE = 851624175280962826449977344.000, MAE = 924687828154.035, R² = -81819020718037872.000\n",
      "Degree 7 - test : MSE = 851671108105784995803561984.000, MAE = 931744477101.772, R² = -35667694184444160.000\n",
      "Degree 8 - validation : MSE = 704313777049536038325765275648.000, MAE = 26592151967679.633, R² = -67666307731815833600.000\n",
      "Degree 8 - test : MSE = 704313778404283686220942278656.000, MAE = 26593320518348.742, R² = -29496419708174581760.000\n",
      "Degree 9 - validation : MSE = 33764580030015277326457974554624.000, MAE = 184120017476478.969, R² = -3243901421774754021376.000\n",
      "Degree 9 - test : MSE = 33764580036352179772140315213824.000, MAE = 184122596938919.250, R² = -1414049042571504910336.000\n",
      "Degree 10 - validation : MSE = 30030318787785529476471395975168.000, MAE = 173640210073227.250, R² = -2885135657705454698496.000\n",
      "Degree 10 - test : MSE = 30030318788019905808279011328000.000, MAE = 173640718716087.062, R² = -1257659461026850275328.000\n",
      "\n",
      "RBF Kernel Regression Results:\n",
      "Best RBF MSE on Validation Set: 8883549012.480, MAE = 29924.732, R² = 0.147\n",
      "Best RBF MSE on Test Set: 22431008060.241, MAE = 34508.444, R² = 0.061\n",
      "Best RBF Model Hyperparameters: {'C': 100, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression and Radial Basis Function (RBF) Kernel Regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "#######################################\n",
    "###  Polynomial Regression Results  ###\n",
    "#######################################\n",
    "print(\"Polynomial Regression Results:\")\n",
    "for degree in range(2, 11):  # Degrees from 2 to 10 inclusive\n",
    "    # Transform input features to polynomial features\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_val_poly = poly.transform(X_val_scaled)\n",
    "    X_test_poly = poly.transform(X_test_scaled)\n",
    "    # Train Linear Regression model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Calculate evaluation metrics for validation set\n",
    "    y_val_pred_poly = lin_reg.predict(X_val_poly)\n",
    "    mse_poly = mean_squared_error(y_val, y_val_pred_poly)\n",
    "    mae_poly = mean_absolute_error(y_val, y_val_pred_poly)\n",
    "    r2_poly = r2_score(y_val, y_val_pred_poly)\n",
    "     \n",
    "    # calculate evaluation metrics for test set \n",
    "    y_test_pred_poly = lin_reg.predict(X_test_poly)\n",
    "    mse_poly_test = mean_squared_error(y_test, y_test_pred_poly)\n",
    "    mae_poly_test = mean_absolute_error(y_test, y_test_pred_poly)\n",
    "    r2_poly_test = r2_score(y_test, y_test_pred_poly)\n",
    "    print(f\"Degree {degree} - validation : MSE = {mse_poly:.3f}, MAE = {mae_poly:.3f}, R² = {r2_poly:.3f}\")\n",
    "    print(f\"Degree {degree} - test : MSE = {mse_poly_test:.3f}, MAE = {mae_poly_test:.3f}, R² = {r2_poly_test:.3f}\")\n",
    "\n",
    "#######################################################\n",
    "###  Radial Basis Function (RBF) Kernel Regression  ###\n",
    "#######################################################\n",
    "print(\"\\nRBF Kernel Regression Results:\")\n",
    "\n",
    "# Define hyperparameter grid for SVR\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "rbf_svr = SVR(kernel='rbf')\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(rbf_svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best RBF model from grid search\n",
    "best_rbf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_rbf = best_rbf_model.predict(X_val_scaled)\n",
    "\n",
    "\n",
    "y_val_pred_rbf_test = best_rbf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for RBF\n",
    "mse_rbf = mean_squared_error(y_val, y_val_pred_rbf)\n",
    "mae_rbf = mean_absolute_error(y_val, y_val_pred_rbf)\n",
    "r2_rbf = r2_score(y_val, y_val_pred_rbf)\n",
    "\n",
    "mse_rbf_test = mean_squared_error(y_test ,y_val_pred_rbf_test)\n",
    "mae_rbf_test = mean_absolute_error(y_test, y_val_pred_rbf_test)\n",
    "r2_rbf_test = r2_score(y_test, y_val_pred_rbf_test)\n",
    "\n",
    "print(f\"Best RBF MSE on Validation Set: {mse_rbf:.3f}, MAE = {mae_rbf:.3f}, R² = {r2_rbf:.3f}\")\n",
    "print(f\"Best RBF MSE on Test Set: {mse_rbf_test:.3f}, MAE = {mae_rbf_test:.3f}, R² = {r2_rbf_test:.3f}\")\n",
    "print(\"Best RBF Model Hyperparameters:\", grid_search.best_params_)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T08:14:41.752345045Z",
     "start_time": "2024-11-18T08:14:14.260032798Z"
    }
   },
   "id": "41835691b05a1373"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
