{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T07:41:54.715900131Z",
     "start_time": "2024-11-18T07:41:54.674087986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4979 entries, 0 to 4978\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   car name         4979 non-null   object \n",
      " 1   engine_capacity  4979 non-null   float64\n",
      " 2   cylinder         4979 non-null   int64  \n",
      " 3   horse_power      4979 non-null   int64  \n",
      " 4   top_speed        4979 non-null   int64  \n",
      " 5   brand            4979 non-null   object \n",
      " 6   country          4979 non-null   object \n",
      " 7   num_seats        4979 non-null   int64  \n",
      " 8   Price_USD        4979 non-null   float64\n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 350.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/processed_cars_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4979 entries, 0 to 4978\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   engine_capacity  4979 non-null   float64\n",
      " 1   cylinder         4979 non-null   int64  \n",
      " 2   horse_power      4979 non-null   int64  \n",
      " 3   top_speed        4979 non-null   int64  \n",
      " 4   num_seats        4979 non-null   int64  \n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 194.6 KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "X = df.drop(columns=[\"Price_USD\", \"car name\",'brand', 'country'])  # Drop 'Price_USD' (target) and 'car name' (irrelevant feature)\n",
    "y = df[\"Price_USD\"]\n",
    "\n",
    "# # One-hot encode categorical features (brand, country)\n",
    "# X = pd.get_dummies(X, columns=[], drop_first=True)\n",
    "# X = X.astype(np.float64)\n",
    "\n",
    "# Splitting the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add bias column to scaled datasets\n",
    "X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_val_b = np.c_[np.ones((X_val_scaled.shape[0], 1)), X_val_scaled]\n",
    "X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "X.info()\n",
    "model_results = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T07:41:54.718567896Z",
     "start_time": "2024-11-18T07:41:54.715599023Z"
    }
   },
   "id": "3a561df254a3c68b"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent - MSE: 6736827576.766, MAE: 27708.579, R²: 0.353\n",
      "Coefficients (Gradient Descent): [ 7.12590700e+04 -5.89216503e+01  1.16963734e+04  4.77025659e+04\n",
      "  1.55275470e+04 -3.05710801e+03]\n",
      "Closed-form - MSE: 6886305765.176, MAE: 27971.329, R²: 0.338\n",
      "Coefficients (Closed-form): [71262.14649146   -72.98585132  9613.45568639 50250.67310241\n",
      " 14856.60315043 -2976.84142149]\n"
     ]
    }
   ],
   "source": [
    "# Closed-form solution & Gradient Descent With comparison between the two methods \n",
    "\n",
    "try:\n",
    "    ##############################\n",
    "    #### Run gradient descent ####\n",
    "    ##############################\n",
    "\n",
    "    theta_best_descent = np.zeros(X_train_b.shape[1])  # Includes bias term  \n",
    "    # Gradient Descent Implementation\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "    m = X_train_b.shape[0]  # Number of samples in training set\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gradients = (1 / m) * X_train_b.T @ (X_train_b @ theta_best_descent - y_train)  # Compute gradients\n",
    "        theta_best_descent -= learning_rate * gradients  # Update theta\n",
    "    \n",
    "    # Predictions on validation set\n",
    "    y_val_pred_gd = X_val_b @ theta_best_descent\n",
    "    \n",
    "    # Metrics for Gradient Descent\n",
    "    mse_gd = np.mean((y_val - y_val_pred_gd) ** 2)\n",
    "    mae_gd = np.mean(np.abs(y_val - y_val_pred_gd))\n",
    "    ss_total_gd = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    ss_residual_gd = np.sum((y_val - y_val_pred_gd) ** 2)\n",
    "    r2_gd = 1 - (ss_residual_gd / ss_total_gd)\n",
    "    \n",
    "    print(f\"Gradient Descent - MSE: {mse_gd:.3f}, MAE: {mae_gd:.3f}, R²: {r2_gd:.3f}\")\n",
    "    print(\"Coefficients (Gradient Descent):\", theta_best_descent)\n",
    "    \n",
    "    ##############################\n",
    "    #### Closed-form solution ####\n",
    "    ##############################\n",
    "    theta_best_closed = np.linalg.inv(X_train_b.T @ X_train_b) @ X_train_b.T @ y_train\n",
    "    y_val_pred_closed = X_val_b @ theta_best_closed\n",
    "    \n",
    "    # Metrics for Closed-form Solution\n",
    "    mse_closed = np.mean((y_val - y_val_pred_closed) ** 2)\n",
    "    mae_closed = np.mean(np.abs(y_val - y_val_pred_closed))\n",
    "    ss_total_closed = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    ss_residual_closed = np.sum((y_val - y_val_pred_closed) ** 2)\n",
    "    r2_closed = 1 - (ss_residual_closed / ss_total_closed)\n",
    "    \n",
    "    print(f\"Closed-form - MSE: {mse_closed:.3f}, MAE: {mae_closed:.3f}, R²: {r2_closed:.3f}\")\n",
    "    print(\"Coefficients (Closed-form):\", theta_best_closed)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Error: Singular matrix. Unable to compute the closed-form solution.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T07:41:54.765414333Z",
     "start_time": "2024-11-18T07:41:54.715746656Z"
    }
   },
   "id": "6219790e81045bfa"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "LASSO - MSE: 6884149445.228, MAE: 27933.368, R²: 0.339\n",
      "Ridge - MSE: 6676014578.626, MAE: 27253.356, R²: 0.359\n",
      "LASSO Coefficients: [   -0.          9554.38939557 50231.92653826 14823.27287075\n",
      " -2900.2790271 ]\n",
      "Ridge Coefficients: [  -59.7163599  11570.24263223 46837.48222636 15380.19623224\n",
      " -3190.66651936]\n",
      "\n",
      "Test Metrics:\n",
      "LASSO - MSE: 16768004847.356, MAE: 31647.551, R²: 0.298\n",
      "Ridge - MSE: 16642234451.202, MAE: 30964.481, R²: 0.303\n"
     ]
    }
   ],
   "source": [
    "# LASSO and Ridge Regression with hyperparameter tuning and evaluation\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter grid\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "############################################ \n",
    "###   LASSO Regression with Grid Search  ###\n",
    "############################################ \n",
    "lasso = Lasso(max_iter=10000)\n",
    "lasso_grid = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "\n",
    "############################################ \n",
    "####  Ridge Regression with Grid Search  ###\n",
    "############################################ \n",
    "ridge = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "\n",
    "################################# \n",
    "###   Validation Set Metrics  ###\n",
    "################################# \n",
    "# LASSO Predictions\n",
    "lasso_val_pred = best_lasso.predict(X_val_scaled)\n",
    "lasso_mse = mean_squared_error(y_val, lasso_val_pred)\n",
    "lasso_mae = mean_absolute_error(y_val, lasso_val_pred)\n",
    "lasso_r2 = r2_score(y_val, lasso_val_pred)\n",
    "\n",
    "# Ridge Predictions\n",
    "ridge_val_pred = best_ridge.predict(X_val_scaled)\n",
    "ridge_mse = mean_squared_error(y_val, ridge_val_pred)\n",
    "ridge_mae = mean_absolute_error(y_val, ridge_val_pred)\n",
    "ridge_r2 = r2_score(y_val, ridge_val_pred)\n",
    "\n",
    "# Print Validation Metrics\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"LASSO - MSE: {lasso_mse:.3f}, MAE: {lasso_mae:.3f}, R²: {lasso_r2:.3f}\")\n",
    "print(f\"Ridge - MSE: {ridge_mse:.3f}, MAE: {ridge_mae:.3f}, R²: {ridge_r2:.3f}\")\n",
    "print(\"LASSO Coefficients:\", best_lasso.coef_)\n",
    "print(\"Ridge Coefficients:\", best_ridge.coef_)\n",
    "\n",
    "############################# \n",
    "###  Test Set Evaluation  ###\n",
    "############################# \n",
    "# LASSO Predictions\n",
    "lasso_test_pred = best_lasso.predict(X_test_scaled)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_test_pred)\n",
    "lasso_test_mae = mean_absolute_error(y_test, lasso_test_pred)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_test_pred)\n",
    "\n",
    "# Ridge Predictions\n",
    "ridge_test_pred = best_ridge.predict(X_test_scaled)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_test_pred)\n",
    "ridge_test_mae = mean_absolute_error(y_test, ridge_test_pred)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_test_pred)\n",
    "\n",
    "# Print Test Metrics\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"LASSO - MSE: {lasso_test_mse:.3f}, MAE: {lasso_test_mae:.3f}, R²: {lasso_test_r2:.3f}\")\n",
    "print(f\"Ridge - MSE: {ridge_test_mse:.3f}, MAE: {ridge_test_mae:.3f}, R²: {ridge_test_r2:.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T07:41:54.843269806Z",
     "start_time": "2024-11-18T07:41:54.769498689Z"
    }
   },
   "id": "c10704a95e243285"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Results:\n",
      "Degree 2: MSE = 369793.868, MAE = 30538.249, R² = -12.138\n",
      "Degree 3: MSE = 2817036.256, MAE = 106412.882, R² = -761.415\n",
      "Degree 4: MSE = 262713562.080, MAE = 8340790.995, R² = -6630880.729\n",
      "Degree 5: MSE = 16013607532.955, MAE = 507426844.230, R² = -24636820351.798\n",
      "Degree 6: MSE = 200846154113.995, MAE = 6364173838.010, R² = -3875549926168.537\n",
      "Degree 7: MSE = 29182600557197.824, MAE = 924687828154.035, R² = -81819020718037872.000\n",
      "Degree 8: MSE = 839234041879579.625, MAE = 26592151967679.633, R² = -67666307731815833600.000\n",
      "Degree 9: MSE = 5810729733003874.000, MAE = 184120017476478.969, R² = -3243901421774754021376.000\n",
      "Degree 10: MSE = 5479992590121407.000, MAE = 173640210073227.250, R² = -2885135657705454698496.000\n",
      "\n",
      "RBF Kernel Regression Results:\n",
      "Best RBF MSE on Validation Set: 94252.581, MAE = 29924.732, R² = 0.147\n",
      "Best RBF Model Hyperparameters: {'C': 100, 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression and Radial Basis Function (RBF) Kernel Regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "#######################################\n",
    "###  Polynomial Regression Results  ###\n",
    "#######################################\n",
    "print(\"Polynomial Regression Results:\")\n",
    "for degree in range(2, 11):  # Degrees from 2 to 10 inclusive\n",
    "    # Transform input features to polynomial features\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "    X_val_poly = poly.transform(X_val_scaled)\n",
    "    \n",
    "    # Train Linear Regression model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred_poly = lin_reg.predict(X_val_poly)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse_poly = np.sqrt(mean_squared_error(y_val, y_val_pred_poly))\n",
    "    mae_poly = mean_absolute_error(y_val, y_val_pred_poly)\n",
    "    r2_poly = r2_score(y_val, y_val_pred_poly)\n",
    "    \n",
    "    print(f\"Degree {degree}: MSE = {mse_poly:.3f}, MAE = {mae_poly:.3f}, R² = {r2_poly:.3f}\")\n",
    "\n",
    "#######################################################\n",
    "###  Radial Basis Function (RBF) Kernel Regression  ###\n",
    "#######################################################\n",
    "print(\"\\nRBF Kernel Regression Results:\")\n",
    "\n",
    "# Define hyperparameter grid for SVR\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "rbf_svr = SVR(kernel='rbf')\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(rbf_svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best RBF model from grid search\n",
    "best_rbf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_rbf = best_rbf_model.predict(X_val_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for RBF\n",
    "mse_rbf = np.sqrt(mean_squared_error(y_val, y_val_pred_rbf))\n",
    "mae_rbf = mean_absolute_error(y_val, y_val_pred_rbf)\n",
    "r2_rbf = r2_score(y_val, y_val_pred_rbf)\n",
    "\n",
    "print(f\"Best RBF MSE on Validation Set: {mse_rbf:.3f}, MAE = {mae_rbf:.3f}, R² = {r2_rbf:.3f}\")\n",
    "print(\"Best RBF Model Hyperparameters:\", grid_search.best_params_)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T07:42:21.495105176Z",
     "start_time": "2024-11-18T07:41:54.846583804Z"
    }
   },
   "id": "41835691b05a1373"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
